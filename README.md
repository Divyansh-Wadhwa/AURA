# A.U.R.A - AI-Based Unified Response Assessment

<p align="center">
  <img src="https://img.shields.io/badge/React-18.2-61DAFB?style=for-the-badge&logo=react" alt="React"/>
  <img src="https://img.shields.io/badge/Node.js-18+-339933?style=for-the-badge&logo=node.js" alt="Node.js"/>
  <img src="https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python" alt="Python"/>
  <img src="https://img.shields.io/badge/FastAPI-0.104-009688?style=for-the-badge&logo=fastapi" alt="FastAPI"/>
  <img src="https://img.shields.io/badge/MongoDB-Atlas-47A248?style=for-the-badge&logo=mongodb" alt="MongoDB"/>
</p>

A comprehensive AI-powered interview practice and soft-skill assessment platform that simulates real interview conversations using an AI interviewer and evaluates users objectively using a **multi-layer ML pipeline** extracting 48+ behavioral features from text, audio, and video signals.

---

## ğŸ“‹ Table of Contents

- [Features](#-features)
- [System Architecture](#-system-architecture)
- [Project Structure](#-project-structure)
- [Tech Stack](#-complete-tech-stack)
- [ML Pipeline Deep Dive](#-ml-pipeline-deep-dive)
  - [Perception Layer](#1-perception-layer-feature-extraction)
  - [Decision Layer](#2-decision-layer-scoring-engine)
- [Feature Extraction Details](#-feature-extraction-details)
- [Getting Started](#-getting-started)
- [API Reference](#-api-reference)
- [Practice Modes](#-practice-modes)
- [Scoring System](#-scoring-system)

---

## ğŸŒŸ Features

| Feature | Description |
|---------|-------------|
| **ğŸ¥ Real-time Video Interviews** | WebRTC-powered video calls with AI avatar interviewer |
| **ğŸ¤– AI Interviewer** | Conversational AI (OpenRouter/Gemini) with context awareness and follow-up questions |
| **ğŸ™ï¸ Voice Interaction** | ElevenLabs text-to-speech for natural AI voice responses |
| **ğŸ“Š 48+ Behavioral Features** | Multi-modal feature extraction from text, audio, and video |
| **ğŸ§  ML-Based Scoring** | XGBoost models trained on behavioral features for objective evaluation |
| **ğŸ‘ï¸ Body Language Detection** | MediaPipe Pose for posture, gestures, and engagement analysis |
| **ğŸ˜Š Facial Expression Analysis** | FER (Facial Expression Recognition) for emotion detection |
| **ğŸ‘€ Gaze & Eye Contact Tracking** | MediaPipe Face Mesh for attention and engagement metrics |
| **ğŸ“ˆ Progress Tracking** | Dashboard with historical trends and analytics |
| **ğŸ” Auth0 Authentication** | Secure OAuth2 authentication with JWT tokens |
| **ğŸ’¬ Multiple Practice Modes** | Text-only, Audio-only, or Full Audio-Video modes |

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              CLIENT (React + Vite)                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Auth0     â”‚  â”‚  3D Avatar  â”‚  â”‚   Video     â”‚  â”‚   Video Perception      â”‚ â”‚
â”‚  â”‚   Context   â”‚  â”‚  (Three.js) â”‚  â”‚   WebRTC    â”‚  â”‚   (Canvas Analysis)     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚ HTTP/WebSocket
                                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           SERVER (Node.js + Express)                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Auth0 JWT  â”‚  â”‚  Socket.IO  â”‚  â”‚  LLM Serviceâ”‚  â”‚   ML Service Bridge     â”‚ â”‚
â”‚  â”‚  Middleware â”‚  â”‚  Real-time  â”‚  â”‚  (OpenRouter)â”‚  â”‚   (Orchestrator)        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                               â”‚
           â–¼                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MongoDB Atlas     â”‚    â”‚              ML PIPELINE                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Users         â”‚  â”‚    â”‚  â”‚  PERCEPTION LAYER   â”‚  â”‚   DECISION LAYER    â”‚   â”‚
â”‚  â”‚ Sessions      â”‚  â”‚    â”‚  â”‚  (FastAPI :5001)    â”‚  â”‚   (FastAPI :8000)   â”‚   â”‚
â”‚  â”‚ Feedback      â”‚  â”‚    â”‚  â”‚                     â”‚  â”‚                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚  â”‚  â€¢ Text Analysis    â”‚  â”‚  â€¢ XGBoost Models   â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚  â€¢ Audio Processing â”‚  â”‚  â€¢ Feature Contract â”‚   â”‚
                           â”‚  â”‚  â€¢ Video Metrics    â”‚  â”‚  â€¢ Score Generation â”‚   â”‚
                           â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

```
User Response â†’ Server â†’ Perception Layer â†’ 48 Features â†’ Decision Layer â†’ 4 Scores â†’ Feedback
     â”‚              â”‚           â”‚                              â”‚
     â”‚              â”‚           â”œâ”€â”€ Text (27 features)         â”œâ”€â”€ Confidence (0-100)
     â”‚              â”‚           â”œâ”€â”€ Audio (14 features)        â”œâ”€â”€ Clarity (0-100)
     â”‚              â”‚           â””â”€â”€ Video (7 features)         â”œâ”€â”€ Empathy (0-100)
     â”‚              â”‚                                          â””â”€â”€ Communication (0-100)
     â”‚              â”‚
     â”‚              â””â”€â”€ LLM (OpenRouter) â†’ AI Response â†’ ElevenLabs TTS â†’ Audio
     â”‚
     â””â”€â”€ Video Metrics (Frontend) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
AURA/
â”œâ”€â”€ client/                      # ğŸ–¥ï¸ Frontend (React + Vite + Tailwind)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ Avatar/          # 3D AI Avatar (Three.js + React Three Fiber)
â”‚   â”‚   â”‚   â”œâ”€â”€ Chat/            # Chat interface components
â”‚   â”‚   â”‚   â”œâ”€â”€ Video/           # Video call components
â”‚   â”‚   â”‚   â”œâ”€â”€ Session/         # Interview session UI
â”‚   â”‚   â”‚   â”œâ”€â”€ Feedback/        # Score display components
â”‚   â”‚   â”‚   â””â”€â”€ common/          # Shared UI components
â”‚   â”‚   â”œâ”€â”€ context/
â”‚   â”‚   â”‚   â”œâ”€â”€ AuthContext.jsx  # Auth0 authentication state
â”‚   â”‚   â”‚   â””â”€â”€ SessionContext.jsx # Interview session state
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”‚   â”œâ”€â”€ useVideoPerception.js  # ğŸ“¹ Client-side video analysis
â”‚   â”‚   â”‚   â””â”€â”€ useAvatarController.js # Avatar animation control
â”‚   â”‚   â”œâ”€â”€ pages/               # Route pages (Dashboard, Interview, Feedback)
â”‚   â”‚   â””â”€â”€ services/
â”‚   â”‚       â”œâ”€â”€ api.js           # Axios API client with Auth0 interceptor
â”‚   â”‚       â””â”€â”€ socket.js        # Socket.IO client
â”‚   â””â”€â”€ public/
â”‚
â”œâ”€â”€ server/                      # âš™ï¸ Backend (Node.js + Express)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”‚   â”œâ”€â”€ db.js            # MongoDB connection
â”‚   â”‚   â”‚   â””â”€â”€ env.js           # Environment configuration
â”‚   â”‚   â”œâ”€â”€ middleware/
â”‚   â”‚   â”‚   â”œâ”€â”€ auth0.middleware.js  # Auth0 JWT validation
â”‚   â”‚   â”‚   â””â”€â”€ error.middleware.js  # Global error handler
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ User.js          # User schema
â”‚   â”‚   â”‚   â”œâ”€â”€ Session.js       # Interview session schema
â”‚   â”‚   â”‚   â””â”€â”€ Feedback.js      # Feedback/scores schema
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ auth.routes.js   # Authentication endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ session.routes.js # Session management
â”‚   â”‚   â”‚   â””â”€â”€ feedback.routes.js # Feedback retrieval
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ llm.service.js   # OpenRouter/Gemini AI integration
â”‚   â”‚   â”‚   â”œâ”€â”€ ml.service.js    # ML pipeline orchestration
â”‚   â”‚   â”‚   â””â”€â”€ speech.service.js # ElevenLabs TTS
â”‚   â”‚   â””â”€â”€ sockets/             # Socket.IO event handlers
â”‚   â””â”€â”€ uploads/                 # Audio file storage
â”‚
â”œâ”€â”€ perception/                  # ğŸ§  Perception Layer (Python + FastAPI)
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py              # FastAPI application
â”‚   â”‚   â”œâ”€â”€ perception/
â”‚   â”‚   â”‚   â”œâ”€â”€ text_perception.py   # ğŸ“ Text feature extraction (27 metrics)
â”‚   â”‚   â”‚   â”œâ”€â”€ video_perception.py  # ğŸ“¹ Video feature extraction (15 metrics)
â”‚   â”‚   â”‚   â”œâ”€â”€ audio_perception.py  # ğŸ™ï¸ Audio feature extraction (14 metrics)
â”‚   â”‚   â”‚   â”œâ”€â”€ semantic_depth.py    # Semantic analysis
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_semantic.py      # LLM-assisted analysis
â”‚   â”‚   â”‚   â””â”€â”€ lexicons/            # Word lists for linguistic analysis
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â””â”€â”€ loader.py        # ML model loading (sentence-transformers)
â”‚   â”‚   â””â”€â”€ routes/
â”‚   â”‚       â””â”€â”€ analyze.py       # /analyze/text, /analyze/video endpoints
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ ml-service/                  # ğŸ¯ Decision Layer (Python + FastAPI)
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py              # FastAPI application
â”‚   â”‚   â”œâ”€â”€ decision/
â”‚   â”‚   â”‚   â”œâ”€â”€ scoring.py       # XGBoost model scoring
â”‚   â”‚   â”‚   â”œâ”€â”€ feature_contract.py  # ğŸ“‹ Frozen feature definitions (48 features)
â”‚   â”‚   â”‚   â””â”€â”€ feature_schema.py    # Feature validation
â”‚   â”‚   â””â”€â”€ models/
â”‚   â”‚       â”œâ”€â”€ confidence_model.pkl # Trained XGBoost model
â”‚   â”‚       â”œâ”€â”€ clarity_model.pkl
â”‚   â”‚       â”œâ”€â”€ empathy_model.pkl
â”‚   â”‚       â”œâ”€â”€ communication_model.pkl
â”‚   â”‚       â””â”€â”€ feature_importance.json
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ start-all.sh                 # ğŸš€ Start all services
â”œâ”€â”€ stop-all.sh                  # ğŸ›‘ Stop all services
â””â”€â”€ MANUAL_STARTUP.md            # Manual startup instructions
```

---

## ğŸ› ï¸ Complete Tech Stack

### Frontend (`client/`)

| Technology | Version | Purpose |
|------------|---------|---------|
| **React** | 18.2 | UI framework |
| **Vite** | 5.0 | Build tool & dev server |
| **Tailwind CSS** | 3.4 | Utility-first styling |
| **React Router** | 6.21 | Client-side routing |
| **Auth0 React SDK** | 2.11 | Authentication |
| **Socket.IO Client** | 4.6 | Real-time communication |
| **Three.js** | 0.182 | 3D graphics |
| **React Three Fiber** | 8.15 | React renderer for Three.js |
| **React Three Drei** | 9.88 | Three.js helpers |
| **Chart.js** | 4.4 | Data visualization |
| **Axios** | 1.6 | HTTP client |
| **Lucide React** | 0.294 | Icon library |

### Backend (`server/`)

| Technology | Version | Purpose |
|------------|---------|---------|
| **Node.js** | 18+ | Runtime |
| **Express** | 4.18 | Web framework |
| **Socket.IO** | 4.6 | WebSocket server |
| **Mongoose** | 8.0 | MongoDB ODM |
| **express-oauth2-jwt-bearer** | 1.7 | Auth0 JWT validation |
| **OpenAI SDK** | 4.20 | OpenRouter API client |
| **Groq SDK** | 0.37 | Groq API client |
| **ElevenLabs** | 1.59 | Text-to-speech |
| **Multer** | 1.4 | File uploads |

### Perception Layer (`perception/`)

| Technology | Version | Purpose |
|------------|---------|---------|
| **FastAPI** | 0.104 | API framework |
| **Uvicorn** | 0.24 | ASGI server |
| **Sentence Transformers** | 2.2 | Text embeddings |
| **Transformers** | 4.36 | NLP models |
| **PyTorch** | 2.2+ | Deep learning |
| **MediaPipe** | 0.10+ | Face mesh & pose detection |
| **FER** | 22.5 | Facial expression recognition |
| **OpenCV** | 4.8+ | Image processing |
| **Librosa** | 0.10 | Audio analysis |
| **NLTK** | 3.8 | Natural language processing |
| **TextBlob** | 0.17 | Sentiment analysis |

### Decision Layer (`ml-service/`)

| Technology | Version | Purpose |
|------------|---------|---------|
| **FastAPI** | 0.109 | API framework |
| **scikit-learn** | 1.4 | ML models (XGBoost) |
| **NumPy** | 1.26 | Numerical computing |
| **Pandas** | 2.1 | Data manipulation |
| **Joblib** | 1.3 | Model serialization |

### Database & Auth

| Technology | Purpose |
|------------|---------|
| **MongoDB Atlas** | Cloud database |
| **Auth0** | OAuth2 authentication |

---

## ğŸ§  ML Pipeline Deep Dive

### 1. Perception Layer (Feature Extraction)

The Perception Layer extracts **raw behavioral features** from multimodal signals without making judgments. It runs on **port 5001**.

#### Text Perception (`text_perception.py`)

Extracts **27 text features** from user responses:

| Category | Features | Description |
|----------|----------|-------------|
| **Semantic** | `semantic_relevance_mean`, `semantic_relevance_std`, `topic_drift_ratio` | How well responses stay on topic |
| **Linguistic** | `avg_sentence_length`, `sentence_length_std`, `avg_response_length_sec`, `response_length_consistency` | Response structure patterns |
| **Confidence** | `assertive_phrase_ratio`, `modal_verb_ratio`, `hedge_ratio`, `filler_word_ratio`, `vague_phrase_ratio` | Language confidence indicators |
| **Semantic Depth** | `information_density`, `specificity_score`, `redundancy_score`, `answer_depth_score` | Content quality metrics |
| **LLM-Assisted** | `llm_confidence_mean`, `llm_clarity_mean`, `llm_depth_mean`, `llm_empathy_mean`, `llm_evasion_mean` | AI-inferred semantic qualities |
| **Empathy** | `empathy_phrase_ratio`, `reflective_response_ratio`, `question_back_ratio` | Emotional intelligence signals |
| **Sentiment** | `avg_sentiment`, `sentiment_variance`, `negative_spike_count` | Emotional tone analysis |

#### Video Perception (`video_perception.py`)

Extracts **15 video features** using MediaPipe:

| Category | Features | Description |
|----------|----------|-------------|
| **Gaze & Attention** | `eye_contact_ratio`, `gaze_variance`, `head_turn_frequency` | Where the user is looking |
| **Facial Expression** | `expression_variance`, `smile_ratio`, `neutral_face_ratio`, `emotion_mismatch_score` | Emotional expressions |
| **Body Language** | `body_detected_ratio`, `shoulder_openness`, `gesture_frequency`, `posture_stability`, `forward_lean`, `hand_to_face_ratio`, `arm_cross_ratio`, `gesture_amplitude` | Posture and gestures |

**Technologies Used:**
- **MediaPipe Face Mesh** - 468 facial landmarks + iris tracking
- **MediaPipe Pose** - 33 body landmarks for posture analysis
- **FER (Facial Expression Recognition)** - 7 emotion classifications

#### Audio Perception (`audio_perception.py`)

Extracts **14 audio features** using Librosa:

| Category | Features | Description |
|----------|----------|-------------|
| **Speech Rate** | `speech_rate_wpm`, `speech_rate_variance` | Speaking pace |
| **Pauses** | `mean_pause_duration`, `pause_frequency`, `silence_ratio` | Hesitation patterns |
| **Prosody** | `pitch_mean`, `pitch_variance`, `energy_mean`, `energy_variance`, `monotony_score` | Voice characteristics |
| **Emotion** | `audio_confidence_prob`, `audio_nervous_prob`, `audio_calm_prob`, `emotion_consistency` | Vocal emotion signals |

---

### 2. Decision Layer (Scoring Engine)

The Decision Layer takes the **48 features** from Perception and produces **4 skill scores**. It runs on **port 8000**.

#### Feature Contract (`feature_contract.py`)

Defines the **frozen feature order** that models were trained on:

```python
ALL_FEATURES = TEXT_FEATURES (27) + AUDIO_FEATURES (14) + VIDEO_FEATURES (7)
# Total: 48 features

TARGET_LABELS = ["confidence", "clarity", "empathy", "communication"]
```

#### Scoring Process (`scoring.py`)

1. **Feature Vector Construction** - Builds 48-dimensional vector with defaults for missing features
2. **Model Prediction** - XGBoost models predict each skill score (0-100)
3. **Low Feature Detection** - Identifies features below threshold for feedback
4. **Improvement Suggestions** - Maps low features to actionable advice

```python
# Example output
{
    "confidence": 74,
    "clarity": 68,
    "empathy": 81,
    "communication": 72,
    "low_features": ["silence_ratio", "topic_drift_ratio"],
    "improvement_suggestions": [...]
}
```

---

## ğŸ“Š Feature Extraction Details

### Client-Side Video Analysis (`useVideoPerception.js`)

The frontend performs **lightweight video analysis** using canvas-based pixel analysis:

```javascript
// Extracted metrics (sent to server with session end)
{
  video_available: 1,
  face_presence_ratio: 0.95,      // % frames with face detected
  eye_contact_ratio: 0.82,        // % frames looking at camera
  head_motion_variance: 0.03,     // Movement stability
  facial_engagement_score: 0.65,  // Activity level
  body_detected_ratio: 0.90,      // % frames with body visible
  shoulder_openness: 0.72,        // Posture openness (0-1)
  gesture_frequency: 2.5,         // Gestures per second
  posture_stability: 0.88,        // Steadiness (0-1)
  gesture_amplitude: 0.35         // Gesture size (0-1)
}
```

### Server-Side Feature Logging

The server logs all extracted features in categorized sections:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              TEXT FEATURES (27 metrics)                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Semantic Metrics:
    semantic_relevance_mean: 0.7500
    ...
  Linguistic Metrics:
    avg_sentence_length: 15.0000
    ...

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              VIDEO FEATURES (11 metrics)                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Face & Gaze Metrics:
    face_presence_ratio: 0.9500
    eye_contact_ratio: 0.8200
    ...
  Body Language Metrics:
    body_detected_ratio: 0.9000
    shoulder_openness: 0.7200
    gesture_frequency: 2.5000
    ...
```

---

## ğŸš€ Getting Started

### Prerequisites

- **Node.js** 18+
- **Python** 3.10+
- **MongoDB Atlas** account
- **Auth0** account
- **OpenRouter** API key (or OpenAI/Groq)
- **ElevenLabs** API key (optional, for TTS)

### Quick Start (All Services)

```bash
# Clone and navigate
cd AURA

# Start all services (recommended)
./start-all.sh

# Or start manually (see MANUAL_STARTUP.md)
```

### Manual Setup

#### 1. Backend Server

```bash
cd server
npm install
cp .env.example .env
# Edit .env with your credentials
npm run dev
```

**Required Environment Variables:**
```env
PORT=5002
MONGODB_URI=mongodb+srv://...
AUTH0_DOMAIN=your-tenant.auth0.com
AUTH0_AUDIENCE=https://my-api
OPENROUTER_API_KEY=sk-or-...
ELEVENLABS_API_KEY=...
PERCEPTION_SERVICE_URL=http://localhost:5001
DECISION_SERVICE_URL=http://localhost:8000
```

#### 2. Frontend Client

```bash
cd client
npm install
cp .env.example .env
# Edit .env with Auth0 credentials
npm run dev
```

**Required Environment Variables:**
```env
VITE_API_URL=http://localhost:5002/api
VITE_SOCKET_URL=http://localhost:5002
VITE_AUTH0_DOMAIN=your-tenant.auth0.com
VITE_AUTH0_CLIENT_ID=your-client-id
VITE_AUTH0_AUDIENCE=https://my-api
```

#### 3. Perception Layer

```bash
cd perception
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
uvicorn app.main:app --host 0.0.0.0 --port 5001 --reload
```

#### 4. Decision Layer

```bash
cd ml-service
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

### Service URLs

| Service | URL | Health Check |
|---------|-----|--------------|
| Frontend | http://localhost:5173 | - |
| Backend | http://localhost:5002 | `/health` |
| Perception | http://localhost:5001 | `/health` |
| Decision | http://localhost:8000 | `/health` |

---

## ğŸ“ API Reference

### Authentication (Auth0)

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/auth/sync` | POST | Sync Auth0 user to database |
| `/api/auth/me` | GET | Get current user profile |

### Sessions

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/session/start` | POST | Start new interview session |
| `/api/session/:id/message` | POST | Send message to AI |
| `/api/session/:id/audio` | POST | Send audio message |
| `/api/session/:id/end` | POST | End session (triggers ML analysis) |
| `/api/session/list` | GET | Get user's sessions |
| `/api/session/stats` | GET | Get user statistics |

### Feedback

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/feedback/:sessionId` | GET | Get session feedback & scores |
| `/api/feedback/trends` | GET | Get progress trends |

### ML Services

#### Perception Layer (`:5001`)

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/analyze/text` | POST | Extract text features |
| `/analyze/video` | POST | Extract video features |
| `/health` | GET | Service health check |

#### Decision Layer (`:8000`)

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/score` | POST | Score features â†’ 4 skill scores |
| `/health` | GET | Service health check |

---

## ğŸ“± Practice Modes

| Mode | Description | Features Extracted |
|------|-------------|-------------------|
| **Text Only** | Chat-based interview | Text features (27) |
| **Audio Only** | Voice-based interview | Text (27) + Audio (14) |
| **Audio & Video** | Full video call | Text (27) + Audio (14) + Video (7) |

---

## ğŸ“Š Scoring System

### Output Scores (0-100)

| Score | Description | Key Features |
|-------|-------------|--------------|
| **Confidence** | How confidently you express thoughts | `assertive_phrase_ratio`, `hedge_ratio`, `filler_word_ratio`, `pitch_variance` |
| **Clarity** | Structure and articulation | `avg_sentence_length`, `information_density`, `speech_rate_wpm` |
| **Empathy** | Emotional intelligence | `empathy_phrase_ratio`, `reflective_response_ratio`, `avg_sentiment` |
| **Communication** | Overall effectiveness | Weighted combination of all features |

### Feedback Generation

Low-scoring features are mapped to actionable improvement suggestions:

```javascript
{
  "scores": { "confidence": 65, "clarity": 72, "empathy": 58, "communication": 68 },
  "low_features": ["hedge_ratio", "empathy_phrase_ratio"],
  "suggestions": [
    "Reduce hedging phrases like 'maybe', 'I think', 'sort of'",
    "Show more understanding by acknowledging the interviewer's points"
  ]
}
```

---

## ğŸ”Œ Socket Events

### Client â†’ Server

| Event | Payload | Description |
|-------|---------|-------------|
| `join-room` | `{ sessionId }` | Join interview room |
| `leave-room` | `{ sessionId }` | Leave interview room |
| `ai-speaking` | `{ speaking: boolean }` | AI speech state |

### Server â†’ Client

| Event | Payload | Description |
|-------|---------|-------------|
| `room-joined` | `{ sessionId }` | Confirm room join |
| `ai-response` | `{ text, audioUrl }` | AI response with TTS |
| `error` | `{ message }` | Error notification |

---

## ğŸ¯ Interview Scenarios

- **Technical Interview** - Software engineering concepts
- **Behavioral Interview** - STAR method questions
- **HR Interview** - Career goals and company fit
- **Case Study** - Business analysis scenarios
- **General Practice** - Mixed question types

---

## ğŸ“„ License

MIT License - feel free to use for hackathons, learning, and development.

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

<p align="center">
  Built with â¤ï¸ for interview practice and soft-skill development
</p>
