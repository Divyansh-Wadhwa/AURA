# A.U.R.A - AI-Based Unified Response Assessment

A local-only, MERN-based AI interview and soft-skill assessment system that simulates real interview conversations using an AI interviewer and evaluates users objectively using machine learning-based scoring.

## ğŸŒŸ Features

- **Real-time Video Interviews** - WebRTC-powered video calls
- **AI Interviewer** - Conversational AI that maintains context and asks follow-up questions
- **Multiple Practice Modes** - Text-only, Audio-only, or Audio-Video
- **Objective Scoring** - ML-based evaluation (confidence, clarity, empathy, communication)
- **Explainable Feedback** - Understand exactly what to improve
- **Progress Tracking** - Dashboard with trends and analytics

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Frontend     â”‚â”€â”€â”€â”€â–¶â”‚     Backend     â”‚â”€â”€â”€â”€â–¶â”‚   ML Service    â”‚
â”‚  React + Vite   â”‚     â”‚  Node + Express â”‚     â”‚  Python + Fast  â”‚
â”‚   + Tailwind    â”‚â—€â”€â”€â”€â”€â”‚   + Socket.IO   â”‚â—€â”€â”€â”€â”€â”‚      API        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   MongoDB Atlas â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
AURA/
â”œâ”€â”€ client/                 # Frontend (React + Vite + Tailwind)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/    # UI components
â”‚   â”‚   â”œâ”€â”€ pages/         # Page components
â”‚   â”‚   â”œâ”€â”€ context/       # React context providers
â”‚   â”‚   â”œâ”€â”€ hooks/         # Custom hooks
â”‚   â”‚   â”œâ”€â”€ services/      # API, Socket, WebRTC services
â”‚   â”‚   â””â”€â”€ utils/         # Utilities and constants
â”‚   â””â”€â”€ public/
â”‚
â”œâ”€â”€ server/                 # Backend (Node + Express + Socket.IO)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ config/        # DB and environment config
â”‚   â”‚   â”œâ”€â”€ controllers/   # Route controllers
â”‚   â”‚   â”œâ”€â”€ middleware/    # Auth and error middleware
â”‚   â”‚   â”œâ”€â”€ models/        # Mongoose models
â”‚   â”‚   â”œâ”€â”€ routes/        # API routes
â”‚   â”‚   â”œâ”€â”€ services/      # LLM, ML, Speech services
â”‚   â”‚   â””â”€â”€ sockets/       # Socket.IO handlers
â”‚   â””â”€â”€ uploads/           # Audio uploads (created at runtime)
â”‚
â””â”€â”€ ml-service/            # ML Service (to be implemented)
```

## ğŸš€ Getting Started

### Prerequisites

- Node.js 18+ 
- npm or yarn
- MongoDB Atlas account
- OpenAI API key (for AI interviewer)

### 1. Clone and Setup

```bash
cd AURA
```

### 2. Backend Setup

```bash
cd server

# Install dependencies
npm install

# Create environment file
cp .env.example .env
```

Edit `.env` with your credentials:
```env
PORT=5000
NODE_ENV=development

# MongoDB Atlas - Get from MongoDB Atlas dashboard
MONGODB_URI=mongodb+srv://<username>:<password>@<cluster>.mongodb.net/aura?retryWrites=true&w=majority

# JWT Secret - Generate a random 32+ character string
JWT_SECRET=your_secure_jwt_secret_minimum_32_characters

# OpenAI API Key - Get from OpenAI dashboard
OPENAI_API_KEY=sk-your-openai-api-key

# ML Service (for later)
ML_SERVICE_URL=http://localhost:8000

# Frontend URL
CLIENT_URL=http://localhost:5173
```

Start the backend:
```bash
npm run dev
```

### 3. Frontend Setup

```bash
cd client

# Install dependencies
npm install

# Create environment file (optional - defaults work for local dev)
cp .env.example .env
```

Start the frontend:
```bash
npm run dev
```

### 4. Access the Application

- **Frontend**: http://localhost:5173
- **Backend API**: http://localhost:5000
- **Health Check**: http://localhost:5000/health

## ğŸ”‘ Required API Keys & Configuration

### MongoDB Atlas
1. Create a free account at [MongoDB Atlas](https://www.mongodb.com/atlas)
2. Create a new cluster
3. Create a database user with read/write permissions
4. Get your connection string and replace `<username>`, `<password>`, and `<cluster>` in `.env`
5. Whitelist your IP address (or use 0.0.0.0/0 for development)

### OpenAI API Key
1. Create an account at [OpenAI](https://platform.openai.com)
2. Go to API Keys section
3. Create a new secret key
4. Add it to your `.env` as `OPENAI_API_KEY`

> **Note**: Without the OpenAI API key, the AI interviewer will use fallback responses (still functional for testing).

### JWT Secret
Generate a secure random string (minimum 32 characters):
```bash
node -e "console.log(require('crypto').randomBytes(32).toString('hex'))"
```

## ğŸ“± Practice Modes

| Mode | Description | Requirements |
|------|-------------|--------------|
| **Text Only** | Chat-based interview | None |
| **Audio Only** | Voice-based interview | Microphone permission |
| **Audio & Video** | Full video call | Camera & microphone permissions |

## ğŸ¯ Interview Scenarios

- **Technical Interview** - Software engineering concepts
- **Behavioral Interview** - STAR method questions
- **HR Interview** - Career goals and company fit
- **Case Study** - Business analysis scenarios
- **General Practice** - Mixed question types

## ğŸ“Š Scoring Metrics

| Metric | Description |
|--------|-------------|
| **Confidence** | How confidently you express thoughts |
| **Clarity** | Structure and articulation of responses |
| **Empathy** | Emotional intelligence and understanding |
| **Communication** | Overall communication effectiveness |

## ğŸ› ï¸ Tech Stack

### Frontend
- React 18 + Vite
- Tailwind CSS
- Socket.IO Client
- Chart.js
- Lucide React Icons

### Backend
- Node.js + Express
- Socket.IO
- MongoDB + Mongoose
- JWT Authentication
- OpenAI API

### Real-time Communication
- WebRTC (peer-to-peer media)
- Socket.IO (signaling)
- MediaRecorder API (audio capture)

## ğŸ“ API Endpoints

### Authentication
- `POST /api/auth/register` - Register new user
- `POST /api/auth/login` - Login user
- `GET /api/auth/me` - Get current user

### Sessions
- `POST /api/session/start` - Start interview session
- `POST /api/session/:id/message` - Send message to AI
- `POST /api/session/:id/end` - End session
- `GET /api/session/list` - Get user sessions
- `GET /api/session/stats` - Get user statistics

### Feedback
- `GET /api/feedback/:id` - Get session feedback
- `GET /api/feedback/trends` - Get progress trends

## ğŸ”Œ Socket Events

### Client â†’ Server
- `join-room` - Join interview room
- `offer` / `answer` / `ice-candidate` - WebRTC signaling
- `audio-chunk` - Stream audio data
- `leave-room` - Leave interview room

### Server â†’ Client
- `room-joined` - Confirm room join
- `user-joined` / `user-left` - Participant updates
- `offer` / `answer` / `ice-candidate` - WebRTC signaling

## ğŸš§ Coming Soon (ML Service)

The ML service for objective scoring will include:
- Speech-to-text (Whisper)
- Audio feature extraction (librosa)
- Text analysis and embeddings
- Custom scoring model (XGBoost)

## ğŸ“„ License

MIT License - feel free to use for hackathons, learning, and development.

---

Built for hackathon/demo environments. Runs entirely locally without Docker or deployment pipelines.
